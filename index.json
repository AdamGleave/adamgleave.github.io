[{"authors":["AaronTucker"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"dc406be0fd2f12ba4a472ab24be1ace1","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Aaron Tucker","type":"authors"},{"authors":["AdamGleave"],"categories":null,"content":"I am an artificial intelligence (AI) PhD candidate at UC Berkeley, advised by Stuart Russell. My goal is to develop techniques necessary for advanced automated systems to verifiably act according to human preferences, even in situations unanticipated by their designer. I am particularly interested in improving methods for value learning, and robustness of deep RL.\nI work closely with the Center for Human-Compatible AI, a cross-disciplinary research centre. Prior to joining Berkeley, I had the pleasure of working with Zoubin Ghahramani and Christian Steinruecken during my Master\u0026rsquo;s degree in the Machine Learning Group at the University of Cambridge. Please see my CV for a more comprehensive list of my prior experience.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594088090,"objectID":"93e12eca0a1cf520e3ba3783cae166fe","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am an artificial intelligence (AI) PhD candidate at UC Berkeley, advised by Stuart Russell. My goal is to develop techniques necessary for advanced automated systems to verifiably act according to human preferences, even in situations unanticipated by their designer.","tags":null,"title":"Adam Gleave","type":"authors"},{"authors":["ChristianSteinruecken"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"abd45ec1fee88028e634200371bffdd8","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Christian Steinruecken","type":"authors"},{"authors":["CodyWild"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"9b505797ffd885d7224d9e7fd9ad0b34","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Cody Wild","type":"authors"},{"authors":["DylanHadfieldMenell"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"b2269b9f1fba7e2549e3203d405d0ce4","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Dylan Hadfield-Menell","type":"authors"},{"authors":["IonelGog"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"04e8428f846d47a164dd7fa4e455ba34","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Ionel Gog","type":"authors"},{"authors":["JanLeike"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"47a66ef93ba0c3eeb9b2fe66c16c8907","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Jan Leike","type":"authors"},{"authors":["MalteSchwarzkopf"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"cab2f97c225b1f007b00c5a4f27303a6","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Malte Schwarzkopf","type":"authors"},{"authors":["MichaelDennis"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"f90ef86318c705b936a8a7ad7ea13d1a","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Michael Dennis","type":"authors"},{"authors":["NeelKant"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"3db53a8e70d8f7156ca88aee3d931df4","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Neel Kant","type":"authors"},{"authors":["OliverHabryka"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"7eee1a5445e577f5bb7cf41ce256862c","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Oliver Habryka","type":"authors"},{"authors":["RobertWatson"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"e3cef9ae48311deb4e5bf051ecd16409","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Robert Watson","type":"authors"},{"authors":["Rohin Shah"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"a00c131ed93e0330f9a274a56fb9636b","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Rohin Shah","type":"authors"},{"authors":["Sergey Levine"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"0a852cccb0f636dd961ce15ceec5f060","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Sergey Levine","type":"authors"},{"authors":["ShaneLegg"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"5a878ce3b4a51724cb4ad05e855cc37b","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Shane Legg","type":"authors"},{"authors":["SorenMindermann"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"21883ef61074aa1c5e9397b6ec9a9ed6","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Sören Mindermann","type":"authors"},{"authors":["StevenHand"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"1654de093fe1dad28754eb37a600ab00","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Steven Hand","type":"authors"},{"authors":["StuartRussell"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1594166737,"objectID":"85858b625e97c85bfc4b9277a71b771d","permalink":"https://gleave.me/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"","tags":null,"title":"Stuart Russell","type":"authors"},{"authors":["Adam Gleave","Michael Dennis","Shane Legg","Stuart Russell","Jan Leike"],"categories":null,"content":"","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166737,"objectID":"c469edaeeccd97e5ecd07654375ed555","permalink":"https://gleave.me/publication/2020-06-quantifying-differences/","publishdate":"2020-06-24T00:00:00Z","relpermalink":"/publication/2020-06-quantifying-differences/","section":"publication","summary":"For many tasks, the reward function is too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by examining rollouts from a policy optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences, and the reinforcement learning algorithm failing to optimize the learned reward. Moreover, the rollout method is highly sensitive to details of the environment the learned reward is evaluated in, which often differ in the deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without training a policy. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be precisely approximated and is more robust than baselines to the choice of visitation distribution. Finally, we find that the EPIC distance of learned reward functions to the ground-truth reward is predictive of the success of training a policy, even in different transition dynamics. Our source code is available at https://github.com/HumanCompatibleAI/evaluating-rewards/.","tags":null,"title":"Quantifying Differences in Reward Functions","type":"publication"},{"authors":["Adam Gleave","Michael Dennis","Cody Wild","Neel Kant","Sergey Levine","Stuart Russell"],"categories":null,"content":"","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594169411,"objectID":"289cdf242f098b48f13221ed6d225578","permalink":"https://gleave.me/publication/2020-04-adversarial-policies/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/publication/2020-04-adversarial-policies/","section":"publication","summary":"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.","tags":null,"title":"Adversarial Policies: Attacking Deep Reinforcement Learning","type":"publication"},{"authors":["Aaron Tucker","Adam Gleave","Stuart Russell"],"categories":null,"content":"","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166737,"objectID":"80eabed0dfc9aada3fd6baac9db092d2","permalink":"https://gleave.me/publication/2018-12-irl-video-games/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/publication/2018-12-irl-video-games/","section":"publication","summary":"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.","tags":null,"title":"Inverse Reinforcement Learning for Video Games","type":"publication"},{"authors":["Adam Gleave","Oliver Habryka"],"categories":null,"content":"","date":1531612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166737,"objectID":"6e900e7b012e313b4f8b4aca8a1a62ff","permalink":"https://gleave.me/publication/2018-06-multi-task-irl/","publishdate":"2018-07-15T00:00:00Z","relpermalink":"/publication/2018-06-multi-task-irl/","section":"publication","summary":"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work","tags":null,"title":"Multi-task Maximum Causal Entropy Inverse Reinforcement Learning","type":"publication"},{"authors":["Sören Mindermann","Rohin Shah","Adam Gleave","Dylan Hadfield-Menell"],"categories":null,"content":"","date":1531526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166737,"objectID":"efba9859261dcac58a40f3d6880f62b8","permalink":"https://gleave.me/publication/2018-06-active-ird/","publishdate":"2018-07-14T00:00:00Z","relpermalink":"/publication/2018-06-active-ird/","section":"publication","summary":"Reward design, the problem of selecting an appropriate reward function for an AI system, is both critically important, as it encodes the task the system should perform, and challenging, as it requires reasoning about and understanding the agent’s environment in detail. AI practitioners often iterate on the reward function for their systems in a trial-and-error process to get their desired behavior. Inverse reward design (IRD) is a preference inference method that infers a true reward function from an observed, possibly misspecified, proxy reward function. This allows the system to determine when it should trust its observed reward function and respond appropriately. This has been shown to avoid problems in reward design such as negative side-effects (omitting a seemingly irrelevant but important aspect of the task) and reward hacking (learning to exploit unanticipated loopholes). In this paper, we actively select the set of proxy reward functions available to the designer. This improves the quality of inference and simplifies the associated reward design problem. We present two types of queries: discrete queries, where the system designer chooses from a discrete set of reward functions, and feature queries, where the system queries the designer for weights on a small set of features. We evaluate this approach with experiments in a personal shopping assistant domain and a 2D navigation domain. We find that our approach leads to reduced regret at test time compared with vanilla IRD. Our results indicate that actively selecting the set of available reward functions is a promising direction to improve the efficiency and effectiveness of reward design.","tags":null,"title":"Active Inverse Reward Design","type":"publication"},{"authors":["Adam Gleave","Christian Steinruecken"],"categories":null,"content":"","date":1490486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166737,"objectID":"9e12ff34887234a3a64cba8e16456a33","permalink":"https://gleave.me/publication/2017-unicode-compression/","publishdate":"2017-03-26T00:00:00Z","relpermalink":"/publication/2017-unicode-compression/","section":"publication","summary":"The majority of online content is written in languages other than English, and is most commonly encoded in UTF-8, the world’s dominant Unicode character encoding. Traditional compression algorithms typically operate on individual bytes. While this approach works well for the single-byte ASCII encoding, it works poorly for UTF-8, where characters often span multiple bytes. Our paper introduces a technique to modify byte-by-byte compressors to operate directly on Unicode characters. We demonstrate this technique applied to LZW and PPM, finding our variant substantially outperforms the original unmodified compressors.","tags":null,"title":"Making Compression Algorithms for Unicode Text","type":"publication"},{"authors":["Ionel Gog","Malte Schwarzkopf","Adam Gleave","Robert Watson","Steven Hand"],"categories":null,"content":"","date":1478044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166737,"objectID":"6dbb17688848e82bc5938bbb88a74310","permalink":"https://gleave.me/publication/2016-firmament/","publishdate":"2016-11-02T00:00:00Z","relpermalink":"/publication/2016-firmament/","section":"publication","summary":"Centralized datacenter schedulers can make high-quality placement decisions when scheduling tasks in a cluster. Today, however, high-quality placements come at the cost of high latency at scale, which degrades response time for interactive tasks and reduces cluster utilization.This paper describes Firmament, a centralized scheduler that scales to over ten thousand machines at subsecond placement latency even though it continuously reschedules all tasks via a min-cost max-flow (MCMF) optimization. Firmament achieves low latency by using multiple MCMF algorithms, by solving the problem incrementally, and via problem-specific optimizations. Experiments with a Google workload trace from a 12,500-machine cluster show that Firmament improves placement latency by 20× over Quincy, a prior centralized scheduler using the same MCMF optimization. Moreover, even though Firmament is centralized, it matches the placement latency of distributed schedulers for workloads of short tasks. Finally, Firmament exceeds the placement quality of four widely-used centralized and distributed schedulers on a real-world cluster, and hence improves batch task response time by 6×.","tags":null,"title":"Firmament: Fast, Centralized Cluster Scheduling at Scale","type":"publication"}]